{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85bb3bf8-bc1a-44b8-bb10-15ec0f48c393",
   "metadata": {},
   "source": [
    "# Fulmine LABS Eyball\n",
    "\r",
    "## Overview\n",
    "This Python code implements a class wrapper around an Anomaly Detection model which can be used to visually check if an image is anomalous or not. The supported architecture for this model is 'Siamese Network'.\n",
    "In order to perform reduce false negatives the code compares the image against a jury of randomly selected known good images of configurable size 'jury_size'.\n",
    "If the number of jurors who vote that the image is simlar to the chosen known good image is below a configurable 'threshold' then the code returns a verdict of 'Anomalous', otherwise it returns a verdict of 'Normal'.\n",
    "If an image path is not specified but screen coordinates are, these will be used instead, enabling direct integration with automated visual checking scripts.\n",
    "\n",
    "One goal is to use this class as part of automating visual checking of a medical image (PACS) production pipeline, although it could theoretically visually check any type of image on which the model has been trained.\n",
    "\n",
    "It also has the capability of describing the images, using GPT-4 Turbo Vision, if an OpenAI key is supplied in the 'Eyball-OpenAI_key.txt' file.\n",
    "\n",
    "## Initialize the Eyball class\n",
    "\n",
    "predictor = ModelPredictor(siamese_model_path, known_good_images_folder, Eyball_key, threshold, jury_size)\n",
    "\n",
    "## Example calls\n",
    "\n",
    "role = \"You are a radiology PACS test engineer, analyzing PACS or test process related image anomalies\"\n",
    "\n",
    "image_description_directive = \"If the image is obviously not a medical image, state *** ANOMALOUS ***. If it is a typical medical image as acquired by an imaging modality with no additions or enhancements, state *** NORMAL ***. Otherwise, if it is a medical image but it also clearly has textual overlays or annotations or digital or image processing artifacts that could have been added by the PACS image viewer technology, describe those features and append *** ANOMALOUS ***.\"\n",
    "\n",
    "verdict = predictor.predict_siamese(test_image_path)\n",
    "\n",
    "actual_description = predictor.describe_image(test_image_path, None, role, image_description_directive)\r\n",
    "\r\n",
    "## Author\r\n",
    "Duncan Henderson\r\n",
    "Fulmine Labs LLC\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91377e08-daaa-42a1-bfb4-07e422ffb5d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Duncan\\anaconda3\\envs\\Eyball\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import cv2\n",
    "from PIL import Image, ImageGrab\n",
    "import logging\n",
    "import random\n",
    "import base64\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Lambda\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81e10613-ac38-499e-92d4-43b77337f087",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "known_good_images_folder = r\"D:\\training_images\\test\\valid\"\n",
    "siamese_model_path = r'models\\lung_ct_siamese_network_weights_043024.h5'\n",
    "Eyball_key=r'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'\n",
    "\n",
    "jury_size=12\n",
    "threshold = 0.5\n",
    "\n",
    "# LLM prompts\n",
    "role = \"You are a radiology PACS test engineer, analyzing PACS or test process related image anomalies\"\n",
    "image_description_directive = \"If the image is obviously not a medical image, state *** ANOMALOUS ***. If it is a typical medical image as acquired by an imaging modality with no additions or enhancements, state *** NORMAL ***. Otherwise, if it is a medical image but it also clearly has textual overlays or annotations or digital or image processing artifacts that could have been added by the PACS image viewer technology, describe those features and append *** ANOMALOUS ***.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "667993ce-d3c3-423e-b555-312b0f543291",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPredictor:\n",
    "\n",
    "    def __init__(self, siamese_model_path, known_good_images_folder, api_key, threshold=0.5, jury_size=12):\n",
    "        self.siamese_model_path = siamese_model_path\n",
    "        self.known_good_images_folder = known_good_images_folder\n",
    "        self.api_key = api_key\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.siamese_model = self.load_siamese_model(siamese_model_path)\n",
    "        self.threshold = threshold\n",
    "        self.jury_size = jury_size\n",
    "        self.known_good_images = self.preload_known_good_images()\n",
    "        self.headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\"\n",
    "        }\n",
    "\n",
    "    def preload_known_good_images(self):\n",
    "        # Your existing method to preload images\n",
    "        print(\"Preloading known good images...\")\n",
    "        image_paths = []\n",
    "        for root, dirs, files in os.walk(self.known_good_images_folder):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    full_path = os.path.join(root, file)\n",
    "                    image_paths.append(full_path)\n",
    "        return image_paths\n",
    "        # Cache known good images if needed here\n",
    "        \n",
    "    # Continue to define ModelPredictor class\n",
    "    def load_siamese_model(self, siamese_model_path):\n",
    "        # Define the base network architecture\n",
    "        def initialize_base_network(input_shape):\n",
    "            input = Input(shape=input_shape)\n",
    "            x = Conv2D(64, (3, 3), activation='relu')(input)\n",
    "            x = MaxPooling2D((2, 2))(x)\n",
    "            x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "            x = MaxPooling2D((2, 2))(x)\n",
    "            x = Flatten()(x)\n",
    "            x = Dense(128, activation='relu')(x)\n",
    "            return Model(input, x)\n",
    "\n",
    "        # Rebuild the Siamese network architecture\n",
    "        input_shape = (152, 152, 1)\n",
    "        base_network = initialize_base_network(input_shape)\n",
    "        input_a = Input(shape=input_shape)\n",
    "        input_b = Input(shape=input_shape)\n",
    "        processed_a = base_network(input_a)\n",
    "        processed_b = base_network(input_b)\n",
    "        distance = Lambda(lambda tensors: tf.sqrt(tf.reduce_sum(tf.square(tensors[0] - tensors[1]), axis=1, keepdims=True)))([processed_a, processed_b])\n",
    "        model = Model([input_a, input_b], distance)\n",
    "        model.load_weights(siamese_model_path)  # Load the saved model or weights\n",
    "        print(\"Siamese model loaded successfully.\")\n",
    "        return model\n",
    "\n",
    "    def predict_siamese(self, image_path=None, coordinates=None):\n",
    "        if coordinates:\n",
    "            # Capture the screen if coordinates are provided\n",
    "            captured_image = self.capture_screen(coordinates)\n",
    "            # Convert the captured image to grayscale and resize it\n",
    "            image = cv2.cvtColor(captured_image, cv2.COLOR_BGR2GRAY)\n",
    "            image = cv2.resize(image, (152, 152))\n",
    "        elif image_path:\n",
    "            # Process the image from file path\n",
    "            image = self.preprocess_image(image_path)\n",
    "        else:\n",
    "            raise ValueError(\"Either image_path or coordinates must be provided.\")\n",
    "        \n",
    "        image = np.expand_dims(image, axis=0)  # Adjust as necessary for the model input\n",
    "    \n",
    "        # Randomly select a subset of known good images to compare against\n",
    "        selected_good_images = random.sample(self.known_good_images, min(self.jury_size, len(self.known_good_images)))\n",
    "        votes = []\n",
    "    \n",
    "        for known_good_image_path in selected_good_images:\n",
    "            known_good_image = self.preprocess_image(known_good_image_path)\n",
    "            known_good_image = np.expand_dims(known_good_image, axis=0)  # Adjust as necessary\n",
    "    \n",
    "            # Prepare the pair\n",
    "            image_pair = [image, known_good_image]\n",
    "    \n",
    "            # Make prediction\n",
    "            prediction_distance = self.siamese_model.predict(image_pair)\n",
    "            is_similar = prediction_distance < self.threshold  # Threshold to determine similarity\n",
    "    \n",
    "            # Debugging output\n",
    "            print(f\"Comparing {image_path if image_path else 'screen capture'} with {known_good_image_path}: Distance = {prediction_distance}, Similar = {is_similar}\")\n",
    "            votes.append(is_similar)\n",
    "    \n",
    "        # Calculate the majority vote\n",
    "        num_similar = sum(votes)\n",
    "        majority_similar = num_similar > len(votes) / 2\n",
    "        print(f\"Total votes for 'Similar': {num_similar}/{len(votes)}. Final verdict: {'Normal' if majority_similar else 'Anomalous'}\")\n",
    "    \n",
    "        return majority_similar\n",
    "\n",
    "    def compare_to_known_images(self, captured_image, threshold=0.5, jury_size=3):\n",
    "        processed_captured_image = self.preprocess_data(captured_image)\n",
    "        verdicts = []\n",
    "\n",
    "        for _ in range(jury_size):\n",
    "            comparison_image = np.random.choice(self.known_good_images)\n",
    "            prediction = self.siamese_model.predict([processed_captured_image, comparison_image])\n",
    "            verdicts.append(prediction < threshold)\n",
    "\n",
    "        # Determine if the majority verdict is 'similar' or 'dissimilar'\n",
    "        final_verdict = sum(verdicts) >= jury_size / 2\n",
    "        return final_verdict\n",
    "    \n",
    "    def evaluate_image(self, coordinates):\n",
    "        # Capture the image from screen coordinates\n",
    "        captured_image = self.capture_screen(coordinates)\n",
    "\n",
    "        # Compare to known images to get a verdict\n",
    "        is_normal = self.compare_to_known_images(captured_image)\n",
    "\n",
    "        return is_normal\n",
    "\n",
    "    def preprocess_image(self, img_path: str, target_size=(152, 152)):\n",
    "        try:\n",
    "            image = load_img(img_path, target_size=target_size, color_mode='grayscale')\n",
    "            image = img_to_array(image)\n",
    "            image /= 255.0  # Normalize to [0, 1]\n",
    "            if image.shape[-1] == 1:  # Check if image is grayscale\n",
    "                image = image.squeeze(-1)  # Remove the channels dimension if grayscale\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Failed to open image at {img_path}: {e}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image at {img_path}: {e}\")\n",
    "            return None\n",
    "        return image \n",
    "\n",
    "    def capture_screen(self, coordinates):\n",
    "        \"\"\" Capture the screen area defined by coordinates. \"\"\"\n",
    "        screenshot = ImageGrab.grab(bbox=coordinates)\n",
    "        return np.array(screenshot, dtype=np.uint8)  # Ensure dtype is uint8\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        \"\"\" Encode image array to base64 string. \"\"\"\n",
    "        if isinstance(image, np.ndarray):\n",
    "            # Convert numpy array to PIL Image if it's not already one\n",
    "            image = Image.fromarray(image.astype('uint8'), 'RGB')\n",
    "        buffer = io.BytesIO()\n",
    "        image.save(buffer, format=\"JPEG\")\n",
    "        return base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "\n",
    "    # def describe_image(self, image_path=None, coordinates=None, role_description=\"User\", image_description_directive=\"Describe the image\"):\n",
    "    #     \"\"\" Describe an image either from a file or from screen coordinates. \"\"\"\n",
    "    #     if image_path:\n",
    "    #         image = self.preprocess_image(image_path)\n",
    "    #     elif coordinates:\n",
    "    #         image = self.capture_screen(coordinates)\n",
    "    #     else:\n",
    "    #         raise ValueError(\"Either image_path or coordinates must be provided.\")\n",
    "    \n",
    "    #     base64_image = self.encode_image(image)\n",
    "    \n",
    "    #     payload = {\n",
    "    #         \"model\": \"gpt-4-turbo\",\n",
    "    #         \"messages\": [\n",
    "    #             {\n",
    "    #                 \"role\": \"system\",\n",
    "    #                 \"content\": role_description\n",
    "    #             },\n",
    "    #             {\n",
    "    #                 \"role\": \"user\",\n",
    "    #                 \"content\": [\n",
    "    #                     {\n",
    "    #                         \"type\": \"text\",\n",
    "    #                         \"text\": image_description_directive\n",
    "    #                     },\n",
    "    #                     {\n",
    "    #                         \"type\": \"image_url\",\n",
    "    #                         \"image_url\": {\n",
    "    #                             \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "    #                         }\n",
    "    #                     }\n",
    "    #                 ]\n",
    "    #             }\n",
    "    #         ],\n",
    "    #         \"max_tokens\": 300\n",
    "    #     }\n",
    "    \n",
    "    #     response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=self.headers, json=payload)\n",
    "    #     if response.status_code != 200:\n",
    "    #         print(\"Error from API:\", response.status_code, response.text)\n",
    "    #         return None\n",
    "    \n",
    "    #     try:\n",
    "    #         description = response.json()['choices'][0]['message']['content']\n",
    "    #         print(\"Image Description:\", description)\n",
    "    #         return description\n",
    "    #     except KeyError as e:\n",
    "    #         print(\"Failed to parse API response:\", response.json())\n",
    "    #         raise e\n",
    "\n",
    "    def preprocess_and_encode_image(self, image_path=None, coordinates=None):\n",
    "        \"\"\" Load an image from a path or capture screen, preprocess, and encode it. \"\"\"\n",
    "        if image_path:\n",
    "            image = self.preprocess_image(image_path)\n",
    "        elif coordinates:\n",
    "            image = self.capture_screen(coordinates)\n",
    "            image = self.preprocess_image(image)  # Assuming preprocess can handle numpy arrays directly\n",
    "        else:\n",
    "            raise ValueError(\"Either image_path or coordinates must be provided.\")\n",
    "    \n",
    "        base64_image = self.encode_image(image)\n",
    "        return base64_image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def describe_image(self, image_path=None, coordinates=None, role_description=\"User\", image_description_directive=\"Describe the image\"):\n",
    "        if image_path:\n",
    "            image = self.preprocess_image(image_path)\n",
    "        elif coordinates:\n",
    "            image = self.capture_screen(coordinates)\n",
    "        else:\n",
    "            raise ValueError(\"Either image_path or coordinates must be provided.\")\n",
    "    \n",
    "        if image is None:\n",
    "            raise ValueError(\"Failed to load or process image.\")\n",
    "    \n",
    "        # try:\n",
    "        #     #print(\"Image dtype before conversion:\", image.dtype)  # Debugging output\n",
    "        #     #print(\"Image shape before conversion:\", image.shape)  # Debugging output\n",
    "        #     image = Image.fromarray(image.astype('uint8'), 'RGB')\n",
    "        # except ValueError as e:\n",
    "        #     print(\"Error during image encoding:\", e)\n",
    "        #     return None\n",
    "    \n",
    "        base64_image = self.encode_image(image)\n",
    "    \n",
    "    \n",
    "        # Ensure image is properly formatted as a numpy array if not done in preprocess\n",
    "        if not isinstance(image, np.ndarray):\n",
    "            raise ValueError(\"Processed image must be a numpy array.\")\n",
    "    \n",
    "        # Encode the processed image to base64\n",
    "        base64_image = self.encode_image(image)\n",
    "        \n",
    "        # Construct payload\n",
    "        payload = {\n",
    "            \"model\": \"gpt-4-turbo\",\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": role_description\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": image_description_directive\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"max_tokens\": 300\n",
    "        }\n",
    "    \n",
    "        # Send request\n",
    "        response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=self.headers, json=payload)\n",
    "        if response.status_code != 200:\n",
    "            print(\"Error from API:\", response.status_code, response.text)\n",
    "            return None\n",
    "    \n",
    "        try:\n",
    "            description = response.json()['choices'][0]['message']['content']\n",
    "            print(\"Image Description:\", description)\n",
    "            return description\n",
    "        except KeyError as e:\n",
    "            print(\"Failed to parse API response:\", response.json())\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc401056-204c-4548-b9c7-47029d87dc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Siamese model loaded successfully.\n",
      "Preloading known good images...\n"
     ]
    }
   ],
   "source": [
    "predictor = ModelPredictor(siamese_model_path, known_good_images_folder, Eyball_key, threshold, jury_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a472b6e-13fd-44de-a829-6039ab33bdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 64ms/step\n",
      "Comparing screen capture with D:\\training_images\\test\\valid\\zoomed\\randomized_wl\\c7de3c88-90f9-41a7-95b7-e6827e99d95c_0.png: Distance = [[53.70162]], Similar = [[False]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Comparing screen capture with D:\\training_images\\test\\valid\\zoomed\\randomized_wl\\13231a60-86f8-4f4a-b695-f4b252425385_0.png: Distance = [[53.70162]], Similar = [[False]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Comparing screen capture with D:\\training_images\\test\\valid\\dummy_class\\2eb738fa-250b-422b-8ef7-a5a174582c21.png: Distance = [[53.70162]], Similar = [[False]]\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Comparing screen capture with D:\\training_images\\test\\valid\\randomized_wl\\cropped\\e4a3e90d-e038-41ca-a549-adc72c61fb71_1.png: Distance = [[53.60519]], Similar = [[False]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Comparing screen capture with D:\\training_images\\test\\valid\\zoomed\\randomized_wl\\107ff504-d5ac-44fb-94ac-bb36f0651db1_1.png: Distance = [[53.70162]], Similar = [[False]]\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Comparing screen capture with D:\\training_images\\test\\valid\\zoomed\\randomized_wl\\c7fd2b40-c72a-4322-92bf-04a82b00cb9b_1.png: Distance = [[53.70162]], Similar = [[False]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Comparing screen capture with D:\\training_images\\test\\valid\\dummy_class\\d6f90751-febd-4421-9d55-8fc3bb9b7b7e.png: Distance = [[53.70162]], Similar = [[False]]\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Comparing screen capture with D:\\training_images\\test\\valid\\randomized_wl\\cropped\\07a9cef1-8ebd-4af0-b1f7-790929436a86_1.png: Distance = [[53.70162]], Similar = [[False]]\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Comparing screen capture with D:\\training_images\\test\\valid\\zoomed\\randomized_wl\\68e72bde-439f-4b9d-81ef-423d45f5b736_0.png: Distance = [[53.70162]], Similar = [[False]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Comparing screen capture with D:\\training_images\\test\\valid\\randomized_wl\\cropped\\7bc0341e-af37-4893-80b9-33376668f756_0.png: Distance = [[53.70162]], Similar = [[False]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Comparing screen capture with D:\\training_images\\test\\valid\\zoomed\\randomized_wl\\22e442e4-0459-4b82-8f15-9846f5041cc9_0.png: Distance = [[53.70162]], Similar = [[False]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Comparing screen capture with D:\\training_images\\test\\valid\\dummy_class\\e7f9ef36-01b0-4539-8f7b-4061c7c690c1.png: Distance = [[53.70162]], Similar = [[False]]\n",
      "Total votes for 'Similar': [[0]]/12. Final verdict: Anomalous\n",
      "Image Description: *** ANOMALOUS ***\n",
      "\n",
      "This image is a screenshot of a user interface from JupyterLab, a web-based interactive development environment for notebooks, code, and data. It is not a medical image acquired by any imaging modality, hence it is classified as anomalous in the context of medical imagery analysis.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'*** ANOMALOUS ***\\n\\nThis image is a screenshot of a user interface from JupyterLab, a web-based interactive development environment for notebooks, code, and data. It is not a medical image acquired by any imaging modality, hence it is classified as anomalous in the context of medical imagery analysis.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Capture and evaluate an area of the screen ...\n",
    "\n",
    "left = 10\n",
    "right = 500\n",
    "top = 10\n",
    "bottom = 200\n",
    "\n",
    "predictor.predict_siamese(coordinates=(left, top, right, bottom))\n",
    "\n",
    "predictor.describe_image(coordinates=(left, top, right, bottom), role_description=role, image_description_directive=image_description_directive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777f8e1c-44b0-492d-b56a-711023600ceb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Or (from here on) pass in a captured and saved file\n",
    "\n",
    "test_image_path = r'C:\\temp\\engineer_typing3.png'\n",
    "print(\"Model predicts\", predictor.predict_siamese(image_path=test_image_path))\n",
    "\n",
    "actual_description = predictor.describe_image(image_path=test_image_path, role_description=role, image_description_directive=image_description_directive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77b15a7-e3e2-4787-86fb-1f235064923b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = r'D:\\Custom_invalid\\cat.jpg'\n",
    "predictor.predict_siamese(test_image_path)\n",
    "\n",
    "actual_description = predictor.describe_image(test_image_path, None, role, image_description_directive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f01e690-9589-44e2-81a0-1ed654a10ab2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_image_path = r'D:\\custom_test_valid\\internet_27f6574b96deb965217cff1aac35fc_gallery.jpg'\n",
    "print(\"Model predicts\", predictor.predict_siamese(test_image_path))\n",
    "\n",
    "actual_description = predictor.describe_image(test_image_path, None, role, image_description_directive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3207058d-10f5-44cc-9fb3-013e7bcafdc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_image_path = r'D:\\custom_test_valid\\istockphoto-493741910-612x612.jpg'\n",
    "print(\"Model predicts\", predictor.predict_siamese(test_image_path))\n",
    "\n",
    "actual_description = predictor.describe_image(test_image_path, None, role, image_description_directive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23da4cc4-4cfc-4104-9605-9874b562844c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_image_path = r'D:\\custom_test_valid\\low-dose-lung-cancer-screening-with-lung-nodules.jpg'\n",
    "print(\"Model predicts\", predictor.predict_siamese(test_image_path))\n",
    "\n",
    "actual_description = predictor.describe_image(test_image_path, None, role, image_description_directive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a035cf-a390-4bc8-944a-3f1b8ed20866",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = r'D:\\custom_invalid\\istockphoto-with_arrow.jpg'\n",
    "print(\"Model predicts\", predictor.predict_siamese(test_image_path))\n",
    "\n",
    "actual_description = predictor.describe_image(test_image_path, None, role, image_description_directive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed5e3eb-d7b5-4ef5-92a8-6fea7d72c0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = r'D:\\custom_invalid\\Lung_abscess_-_CT_with_overlay.jpg'\n",
    "print(\"Model predicts\", predictor.predict_siamese(test_image_path))\n",
    "\n",
    "actual_description = predictor.describe_image(test_image_path, None, role, image_description_directive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079be922-c1a5-42ec-96b9-2535a2bb4bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = r'D:\\Custom_invalid\\augmented_0abe42cc-623a-46f2-91ee-be4f339ff73b.png'\n",
    "print(\"Model predicts\", predictor.predict_siamese(test_image_path))\n",
    "\n",
    "actual_description = predictor.describe_image(test_image_path, None, role, image_description_directive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015df824-0f4a-44a1-9cb5-6a46e27d5723",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = r'C:\\temp\\medical_image_zoomed_more_resized_modified_aspect_ratio_hairlines.png'\n",
    "print(\"Model predicts\", predictor.predict_siamese(test_image_path))\n",
    "\n",
    "actual_description = predictor.describe_image(test_image_path, None, role, image_description_directive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd5a7a2-e417-4c37-9575-96a642a02aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = r'D:\\Custom_invalid\\internet-gettyimages-1320918955-612x612_small_label.jpg'\n",
    "print(\"Model predicts\", predictor.predict_siamese(test_image_path))\n",
    "\n",
    "\n",
    "actual_description = predictor.describe_image(test_image_path, None, role, image_description_directive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c24324-c0d2-417a-8b98-d66e02cb70a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = r'D:\\Custom_test_valid\\internet-gettyimages-1322138871-612x612.jpg'\n",
    "print(\"Model predicts\", predictor.predict_siamese(test_image_path))\n",
    "\n",
    "actual_description = predictor.describe_image(test_image_path, None, role, image_description_directive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2580e143-b43c-40fc-8788-316f7326d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_methods_simplified(base_folder, sample_size=40, jury_size=12, role=\"User\", image_description_directive=\"Describe the image\"):\n",
    "    valid_folder = os.path.join(base_folder, 'valid')\n",
    "    invalid_folder = os.path.join(base_folder, 'invalid')\n",
    "\n",
    "    # Ensure directories exist\n",
    "    if not os.path.exists(valid_folder) or not os.path.exists(invalid_folder):\n",
    "        raise ValueError(\"One or more image directories do not exist.\")\n",
    "\n",
    "    # List and sample images\n",
    "    valid_images = random.sample(os.listdir(valid_folder), min(sample_size, len(os.listdir(valid_folder))))\n",
    "    invalid_images = random.sample(os.listdir(invalid_folder), min(sample_size, len(os.listdir(invalid_folder))))\n",
    "\n",
    "    # Initialize predictions for both methods\n",
    "    predictions_siamese = []\n",
    "    predictions_gpt = []\n",
    "\n",
    "    # Initialize actual values\n",
    "    actuals = [1] * len(valid_images) + [0] * len(invalid_images)  # 1 for normal, 0 for anomalous\n",
    "\n",
    "    # Process sampled valid and invalid images\n",
    "    for filename in valid_images + invalid_images:\n",
    "        print (\"Filename\", filename)\n",
    "        folder = valid_folder if filename in valid_images else invalid_folder\n",
    "        image_path = os.path.join(folder, filename)\n",
    "\n",
    "        # Siamese Network Prediction\n",
    "        siamese_result = predictor.predict_siamese(image_path)\n",
    "        predictions_siamese.append(siamese_result)\n",
    "        print (\"Siamese result\", siamese_result)\n",
    "\n",
    "        # GPT Vision Direct Analysis Prediction\n",
    "        description_result = predictor.describe_image(image_path, None, role, image_description_directive)\n",
    "        predictions_gpt.append('NORMAL' in description_result)\n",
    "        print (\"API result\", description_result)\n",
    "\n",
    "    # Assuming that True/False predictions from Siamese network are correct and just need flattening:\n",
    "    predictions_siamese = [int(pred.flatten()[0]) for pred in predictions_siamese]\n",
    "    \n",
    "    # Convert GPT predictions from True/False to 0/1 as well:\n",
    "    predictions_gpt = [int(pred) for pred in predictions_gpt]\n",
    "    \n",
    "    # Recalculate the metrics:\n",
    "    accuracy_s = accuracy_score(actuals, predictions_siamese)\n",
    "    precision_s = precision_score(actuals, predictions_siamese)\n",
    "    recall_s = recall_score(actuals, predictions_siamese)\n",
    "    f1_s = f1_score(actuals, predictions_siamese)\n",
    "    \n",
    "    accuracy_g = accuracy_score(actuals, predictions_gpt)\n",
    "    precision_g = precision_score(actuals, predictions_gpt)\n",
    "    recall_g = recall_score(actuals, predictions_gpt)\n",
    "    f1_g = f1_score(actuals, predictions_gpt)\n",
    "    \n",
    "    print('Evaluation Results - Siamese Model:', {\n",
    "        'accuracy': accuracy_s,\n",
    "        'precision': precision_s,\n",
    "        'recall': recall_s,\n",
    "        'f1': f1_s\n",
    "    })\n",
    "    print('Evaluation Results - GPT Model:', {\n",
    "        'accuracy': accuracy_g,\n",
    "        'precision': precision_g,\n",
    "        'recall': recall_g,\n",
    "        'f1': f1_g\n",
    "    })\n",
    "\n",
    "    return {\n",
    "        \"siamese\": {\"accuracy\": accuracy_s, \"precision\": precision_s, \"recall\": recall_s, \"f1\": f1_s},\n",
    "        \"gpt\": {\"accuracy\": accuracy_g, \"precision\": precision_g, \"recall\": recall_g, \"f1\": f1_g}\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f7cba-4e1b-4dca-9f64-07ba42b0836d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example call\n",
    "base_folder = r'D:\\model_comparison_test'\n",
    "sample_size = 40\n",
    "try:\n",
    "    results = evaluate_methods_simplified(base_folder, sample_size, jury_size, role, image_description_directive)\n",
    "    print(\"Evaluation Results:\", results)\n",
    "except Exception as e:\n",
    "    print(\"Error during evaluation:\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f20f82-139c-432a-9e6d-557674431f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deedc117-f248-46ff-b365-bf0538982ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
